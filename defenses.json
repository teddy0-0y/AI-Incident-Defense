[
    {
        "defense_id":"AID-DV-001",
        "name":"Honeypot AI Services & Decoy Models\/APIs",
        "description":"Deploy decoy AI systems, such as fake LLM APIs, ML model endpoints serving synthetic or non-sensitive data, or imitation agent services, that are designed to appear valuable, vulnerable, or legitimate to potential attackers. These honeypots are instrumented for intensive monitoring to log all interactions, capture attacker TTPs (Tactics, Techniques, and Procedures), and gather threat intelligence without exposing real production systems or data. They can also be used to slow down attackers or waste their resources.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-002",
        "name":"Honey Data, Decoy Artifacts & Canary Tokens for AI",
        "description":"Strategically seed the AI ecosystem (training datasets, model repositories, configuration files, API documentation) with enticing but fake data, decoy model artifacts (e.g., a seemingly valuable but non-functional or instrumented model file), or canary tokens (e.g., fake API keys, embedded URLs in documents). These \\\"honey\\\" elements are designed to be attractive to attackers. If an attacker accesses, exfiltrates, or attempts to use these decoys, it triggers an alert, signaling a breach or malicious activity and potentially providing information about the attacker's actions or location.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-003",
        "name":"Dynamic Response Manipulation for AI Interactions",
        "description":"Implement mechanisms where the AI system, upon detecting suspicious or confirmed adversarial interaction patterns (e.g., repeated prompt injection attempts, queries indicative of model extraction), deliberately alters its responses to be misleading, unhelpful, or subtly incorrect to the adversary. This aims to frustrate the attacker's efforts, waste their resources, make automated attacks less reliable, and potentially gather more intelligence on their TTPs without revealing the deception. The AI might simultaneously alert defenders to the ongoing deceptive engagement.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-004",
        "name":"AI Output Watermarking & Telemetry Traps",
        "description":"Embed imperceptible or hard-to-remove watermarks, unique identifiers, or telemetry \\\"beacons\\\" into the outputs generated by AI models (e.g., text, images, code). If these outputs are found externally (e.g., on the internet, in a competitor's product, in leaked documents), the watermark or beacon can help trace the output back to the originating AI system, potentially identifying model theft, misuse, or data leakage. Telemetry traps involve designing the AI to produce specific, unique (but benign) outputs for certain rare or crafted inputs, which, if observed externally, indicate that the model or its specific knowledge has been compromised or replicated.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-005",
        "name":"Decoy Agent Behaviors & Canary Tasks",
        "description":"For autonomous AI agents, design and implement decoy or \\\"canary\\\" functionalities, goals, or sub-agents that appear valuable or sensitive but are actually monitored traps. If an attacker successfully manipulates an agent (e.g., via prompt injection or memory poisoning) and directs it towards these decoy tasks or to exhibit certain predefined suspicious behaviors, it triggers an alert, revealing the compromise attempt and potentially the attacker's intentions, without risking real assets.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-006",
        "name":"Deceptive System Information",
        "description":"When probed by unauthenticated or suspicious users, the AI system provides misleading information about its architecture, capabilities, or underlying models. For example, an API might return headers suggesting it's built on a different framework, or an LLM might respond to 'What model are you?' with a decoy answer.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-007",
        "name":"Training-Phase Obfuscation for Model Inversion Defense",
        "description":"A deception technique that defends against model inversion attacks by intentionally adding controlled noise or obfuscation during the model's training phase. By making the relationship between inputs, outputs, and the model's internal parameters less deterministic, the resulting model becomes a 'noisier' and more opaque oracle. This deceives and frustrates an attacker's attempts to reconstruct sensitive training data from the model's outputs.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-DV-008",
        "name":"Poisoning Detection Canaries & Decoy Data",
        "description":"This technique involves proactively embedding synthetic 'canary' or 'sentinel' data points into a training set to deceive and detect data poisoning attacks. These canaries are specifically crafted to be easily learned by the model under normal conditions. During training, the model's behavior on these specific points is monitored. If a data poisoning attack disrupts the overall data distribution or the training process, it will cause an anomalous reaction on these canaries (e.g., a sudden spike in loss, a change in prediction), triggering a high-fidelity alert that reveals the attack without the adversary realizing their method has been detected.",
        "tactic":"Deceive"
    },
    {
        "defense_id":"AID-D-001",
        "name":"Adversarial Input & Prompt Injection Detection",
        "description":"Implement mechanisms to continuously monitor and analyze inputs to AI models, specifically looking for characteristics indicative of adversarial manipulation or malicious prompt content. This includes detecting statistically anomalous inputs (e.g., out-of-distribution samples, inputs with unusual perturbation patterns) and scanning prompts for known malicious patterns, hidden commands, jailbreak sequences, or attempts to inject executable code or harmful instructions. The goal is to block, flag, or sanitize such inputs before they can significantly impact the model's behavior or compromise the system.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-002",
        "name":"AI Model Anomaly & Performance Drift Detection",
        "description":"Continuously monitor the outputs, performance metrics (e.g., accuracy, confidence scores, precision, recall, F1-score, output distribution), and potentially internal states or feature attributions of AI models during operation. This monitoring aims to detect significant deviations from established baselines or expected behavior. Such anomalies or drift can indicate various issues, including concept drift (changes in the underlying data distribution), data drift (changes in input data characteristics), or malicious activities like ongoing data poisoning attacks, subtle model evasion attempts, or model skewing.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-003",
        "name":"AI Output Monitoring & Policy Enforcement",
        "description":"Actively inspect the outputs generated by AI models (e.g., text responses, classifications, agent actions) in near real-time. This involves enforcing predefined safety, security, and ethical policies on the outputs and taking action (e.g., blocking, sanitizing, alerting) when violations are detected.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-004",
        "name":"Model & AI Artifact Integrity Monitoring, Audit & Tamper Detection",
        "description":"Regularly verify the cryptographic integrity and authenticity of deployed AI models, their parameters, associated datasets, and critical components of their runtime environment. This process aims to detect any unauthorized modifications, tampering, or the insertion of backdoors that could compromise the model's behavior, security, or data confidentiality. It ensures that the AI artifacts in operation are the approved, untampered versions.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-005",
        "name":"AI Activity Logging, Monitoring & Threat Hunting",
        "description":"Establish and maintain detailed, comprehensive, and auditable logs of all significant activities related to AI systems. This includes user queries and prompts, model responses and confidence scores, decisions made by AI (especially autonomous agents), tools invoked by agents, data accessed or modified, API calls (to and from the AI system), system errors, and security-relevant events. These logs are then ingested into security monitoring systems (e.g., SIEM) for correlation, automated alerting on suspicious patterns, and proactive threat hunting by security analysts to identify indicators of compromise (IoCs) or novel attack patterns targeting AI systems.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-006",
        "name":"Explainability (XAI) Manipulation Detection",
        "description":"Implement mechanisms to monitor and validate the outputs and behavior of eXplainable AI (XAI) methods. The goal is to detect attempts by adversaries to manipulate or mislead these explanations, ensuring that XAI outputs accurately reflect the model's decision-making process and are not crafted to conceal malicious operations, biases, or vulnerabilities. This is crucial if XAI is used for debugging, compliance, security monitoring, or building user trust.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-007",
        "name":"Multimodal Inconsistency Detection",
        "description":"For AI systems processing multiple input modalities (e.g., text, image, audio, video), implement mechanisms to detect and respond to inconsistencies, contradictions, or malicious instructions hidden via cross-modal interactions. This involves analyzing inputs and outputs across modalities to identify attempts to bypass security controls or manipulate one modality using another, and applying defenses to mitigate such threats.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-008",
        "name":"AI-Based Security Analytics for AI systems",
        "description":"Employ specialized AI\/ML models (secondary AI defenders) to analyze telemetry, logs, and behavioral patterns from primary AI systems to detect sophisticated, subtle, or novel attacks that may evade rule-based or traditional detection methods. This includes identifying anomalous interactions, emergent malicious behaviors, coordinated attacks, or signs of AI-generated attacks targeting the primary AI systems.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-009",
        "name":"Cross-Agent Fact Verification & Hallucination Cascade Prevention",
        "description":"Implement real-time fact verification and consistency checking mechanisms across multiple AI agents to detect and prevent the propagation of hallucinated or false information through agent networks. This technique employs distributed consensus algorithms, external knowledge base validation, and inter-agent truth verification to break hallucination cascades before they spread through the system.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-010",
        "name":"AI Goal Integrity Monitoring & Deviation Detection",
        "description":"Continuously monitor and validate AI agent goals, objectives, and decision-making patterns to detect unauthorized goal manipulation or intent deviation. This technique establishes cryptographically signed goal states, implements goal consistency verification, and provides real-time alerting when agents deviate from their intended objectives or exhibit goal manipulation indicators.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-011",
        "name":"Agent Behavioral Attestation & Rogue Detection",
        "description":"Implement continuous behavioral monitoring and attestation mechanisms to identify rogue or compromised agents in multi-agent systems. This technique uses behavioral fingerprinting, anomaly detection, and peer verification to detect agents that deviate from expected behavioral patterns or exhibit malicious characteristics.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-012",
        "name":"Graph Anomaly & Backdoor Detection",
        "description":"Implements methods to identify malicious nodes, edges, or subgraphs within a graph dataset that are indicative of poisoning or backdoor attacks against Graph Neural Networks.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-013",
        "name":"RL Reward & Policy Manipulation Detection",
        "description":"This technique focuses on monitoring and analyzing Reinforcement Learning (RL) systems to detect two primary threats: reward hacking and reward tampering. Reward hacking occurs when an agent discovers an exploit in the environment's reward function to achieve a high score for unintended or harmful behavior. Reward tampering involves an external actor manipulating the reward signal being sent to the agent. This technique uses statistical analysis of the reward stream and behavioral analysis of the agent's learned policy to detect these manipulations.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-014",
        "name":"RAG Content & Relevance Monitoring",
        "description":"This technique involves the real-time monitoring of a Retrieval-Augmented Generation (RAG) system's behavior at inference time. It focuses on two key checks: 1) Content Analysis, where retrieved document chunks are scanned for harmful content or malicious payloads before being passed to the LLM, and 2) Relevance Analysis, which verifies that the retrieved documents are semantically relevant to the user's original query. A significant mismatch in relevance can indicate a vector manipulation or poisoning attack designed to force the model to use unintended context.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-001.001",
        "name":"Per-Prompt Content & Obfuscation Analysis",
        "description":"Performs real-time analysis on individual prompts to detect malicious content, prompt injection, and jailbreaking attempts. This sub-technique combines two key functions: 1) identifying known malicious patterns and harmful intent using heuristics, regex, and specialized guardrail models, and 2) detecting attempts to hide or obscure these attacks through obfuscation techniques like character encoding (e.g., Base64), homoglyphs, or high-entropy strings. It acts as a primary, synchronous guardrail at the input layer.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-001.002",
        "name":"Synthetic Media & Deepfake Forensics",
        "description":"Detects manipulated or synthetically generated media (e.g., deepfakes) by performing a forensic analysis that identifies a combination of specific technical artifacts and inconsistencies. This technique fuses evidence from multiple indicators across different modalities\u2014such as image compression anomalies, unnatural biological signals (blinking, vocal patterns), audio-visual mismatches, and hidden data payloads\u2014to provide a more robust and reliable assessment of the media's authenticity.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-001.003",
        "name":"Vector-Space Anomaly Detection",
        "description":"Detects semantically novel or anomalous inputs by operating on their vector embeddings rather than their raw content. This technique establishes a baseline of 'normal' inputs by clustering the embeddings of known-good data. At inference time, inputs whose embeddings are statistical outliers or fall far from the normal cluster centroids are flagged as suspicious. This is effective against novel attacks that bypass keyword or pattern-based filters by using unusual but semantically malicious phrasing.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-001.004",
        "name":"LLM Guardrail for Intent\/Privilege Escalation",
        "description":"Use a fast secondary LLM (guardrail) to classify prompts for intent switching, instruction bypass, or privilege escalation before reaching the primary model.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-003.001",
        "name":"Harmful Content & Policy Filtering",
        "description":"Focuses on inspecting AI-generated content for violations of safety and acceptable use policies. This includes detecting hate speech, self-harm content, explicit material, and other categories of harmful or inappropriate output.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-003.002",
        "name":"Sensitive Information & Data Leakage Detection",
        "description":"Focuses on preventing the AI model from inadvertently disclosing sensitive, confidential, or private information in its outputs. This is critical for protecting user privacy and corporate data.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-003.003",
        "name":"Agentic Tool Use & Action Policy Monitoring",
        "description":"This sub-technique focuses on the unique challenge of monitoring the actions of autonomous agents. It involves defining and enforcing strict, machine-readable policies about which tools an agent can use, with what parameters, and in what sequence. The detection mechanism is a real-time policy engine that validates each proposed action against these rules before it is executed, acting as a critical guardrail against unintended or malicious agent behavior.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-003.004",
        "name":"Tool-Call Sequence Anomaly Detection",
        "description":"Model normal tool-call transition probabilities and flag anomalous sequences that deviate from expected agent workflows.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-004.001",
        "name":"Static Artifact Hash & Signature Verification",
        "description":"Periodically re-hash stored models, datasets and container layers and compare against the authorised manifest.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-004.002",
        "name":"Runtime Attestation & Memory Integrity",
        "description":"Attest the running model process (code, weights, enclave MRENCLAVE) to detect in-memory patching or DLL injection.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-004.003",
        "name":"Configuration & Policy Drift Monitoring",
        "description":"Detect unauthorised edits to model-serving YAMLs, feature-store ACLs, RAG index schemas or inference-time policy files.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-004.004",
        "name":"Model Source & Namespace Drift Detection",
        "description":"A set of high-signal detective controls that monitor for symptoms of a model namespace reuse attack or supply chain policy failure. This technique focuses on detecting lifecycle changes in external model repositories (e.g., deletions, redirects) during the curation process and on identifying unexpected network traffic from production systems to public model hubs at runtime.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-005.001",
        "name":"AI System Log Generation & Collection",
        "description":"This foundational technique covers the instrumentation of AI applications to produce detailed, structured logs for all significant events, and the implementation of a secure pipeline to collect and forward these logs to a central analysis platform. The goal is to create a high-fidelity, auditable record of system activity, which is a prerequisite for all other detection, investigation, and threat hunting capabilities.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-005.002",
        "name":"Security Monitoring & Alerting for AI",
        "description":"This technique covers the real-time monitoring of ingested AI system logs and the creation of specific rules to detect and generate alerts for known suspicious or malicious patterns. It focuses on the operational security task of identifying potential threats as they occur by comparing live activity against predefined attack signatures and behavioral heuristics. This is the core function of a Security Operations Center (SOC) in defending AI systems.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-005.003",
        "name":"Proactive AI Threat Hunting",
        "description":"This technique covers the proactive, hypothesis-driven search through AI system logs and telemetry for subtle, unknown, or 'low-and-slow' attacks that do not trigger predefined alerts. Threat hunting assumes an attacker may already be present and evading standard detections. It focuses on identifying novel attack patterns, reconnaissance activities, and anomalous behaviors by using exploratory data analysis, complex queries, and machine learning on historical data.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-005.004",
        "name":"Specialized Agent & Session Logging",
        "description":"This technique covers the highly specialized logging required for autonomous and agentic AI systems, which goes beyond standard API request\/response logging. It involves instrumenting the agent's internal decision-making loop to capture its goals, plans, intermediate thoughts, tool selections, and interactions with memory or knowledge bases. This detailed audit trail is essential for debugging, ensuring compliance, and detecting complex threats like goal manipulation or emergent, unsafe behaviors.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-005.005",
        "name":"Accelerator Telemetry Anomaly Detection",
        "description":"Continuously baseline and monitor accelerator telemetry (power, temperature, utilization, PMCs). Alert on deviations indicating cryptomining, DoS, or side-channel probing.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-011.001",
        "name":"Agent Behavioral Analytics & Anomaly Detection",
        "description":"This data science-driven technique focuses on detecting rogue or compromised agents by analyzing their behavior over time. It involves creating a quantitative 'fingerprint' of an agent's normal operational patterns from logs and telemetry. By continuously comparing an agent's live behavior against its established baseline, this technique can identify significant deviations, drifts, or anomalous patterns that indicate a compromise or hijacking.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-011.002",
        "name":"Inter-Agent Security & Consensus Monitoring",
        "description":"This sub-technique covers the security of agent-to-agent interactions within a multi-agent system. It focuses on implementing mechanisms that allow agents to monitor and validate each other's behavior, report anomalies, and reach consensus before performing critical, system-wide actions. This creates a distributed, peer-to-peer defense layer within the agent ecosystem.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-011.003",
        "name":"Agent Infrastructure & Population Control",
        "description":"This sub-technique covers the infrastructure and orchestration-level controls for managing the agent population and responding to threats. It focuses on a top-down view of the agent ecosystem, ensuring that only authorized agents are running and providing mechanisms to rapidly isolate and contain agents that are confirmed to be rogue or malicious. These are typically automated responses triggered by other detection systems.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-012.001",
        "name":"Discrepancy-Based GNN Backdoor Detection",
        "description":"Detects backdoored nodes in a Graph Neural Network (GNN) by identifying significant discrepancies between a potentially compromised model and a clean baseline model (established via AID-M-003.003). The technique specifically looks for semantic drift (changes in a node's meaning) and attribute over-emphasis (unusual feature importance) caused by the backdoor. Clustering algorithms are then often used to isolate the small group of poisoned nodes based on these detected discrepancies.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-012.002",
        "name":"Structure-Feature Relationship Analysis for GNN Defense",
        "description":"Detects and mitigates training-time adversarial attacks on Graph Neural Networks (GNNs) that perturb the graph structure. The core principle is to analyze the relationship between the graph's connectivity (structure) and the attributes of its nodes (features). By identifying and then pruning or down-weighting anomalous edges that violate expected structure-feature properties (e.g., connecting highly dissimilar nodes), this technique creates a revised, more robust graph for the GNN's message passing, hardening it against structural poisoning.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-012.003",
        "name":"Structural & Topological Anomaly Detection",
        "description":"Detects potential poisoning or backdoor attacks in graphs by analyzing their topological structure, independent of node features. This technique identifies suspicious patterns such as unusually dense subgraphs (cliques), nodes with anomalously high centrality or degree, or other structural irregularities that deviate from the expected properties of the graph and are often characteristic of coordinated attacks.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-014.001",
        "name":"Post-Retrieval Malicious Content Scanning",
        "description":"Treat retrieved RAG chunks as untrusted input; scan for prompt-injection patterns or malicious payloads before inclusion in LLM context.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-014.002",
        "name":"Query-Document Semantic Relevance Verification",
        "description":"Verify cosine similarity between the user query and each candidate chunk using the same embedding model; drop low-similarity items to resist poisoning.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-D-014.003",
        "name":"Source Concentration Monitoring",
        "description":"Alert when top-k retrievals are dominated by a single uncommon source, indicating possible answer drift or targeted source poisoning.",
        "tactic":"Detect"
    },
    {
        "defense_id":"AID-E-001",
        "name":"Credential Revocation & Rotation for AI Systems",
        "description":"Immediately revoke, invalidate, or rotate any credentials (e.g., API keys, access tokens, user account passwords, service account credentials, certificates) that are known or suspected to have been compromised or used by an adversary to gain unauthorized access to or interact maliciously with AI systems, models, data, or MLOps pipelines. This action aims to cut off the attacker's current access and prevent them from reusing stolen credentials.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-002",
        "name":"AI Process & Session Eviction",
        "description":"Terminate any running AI model instances, agent processes, user sessions, or containerized workloads that are confirmed to be malicious, compromised, or actively involved in an attack. This immediate action halts the adversary's ongoing activities within the AI system and removes their active foothold.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-003",
        "name":"AI Backdoor & Malicious Artifact Removal",
        "description":"Systematically scan for, identify, and remove any malicious artifacts introduced by an attacker into the AI system. This includes backdoors in models, poisoned data, malicious code, or configuration changes designed to grant persistent access or manipulate AI behavior.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-004",
        "name":"User-Agent",
        "description":"After an attack vector has been identified and the adversary evicted, rapidly apply necessary security patches to vulnerable software components (e.g., ML libraries, operating systems, web servers, agent frameworks) and harden system configurations that were exploited or found to be weak. This step aims to close the specific vulnerabilities used by the attacker and strengthen overall security posture to prevent reinfection or similar future attacks.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-005",
        "name":"Compromised Session Termination & State Purging",
        "description":"When communication channels or user\/agent sessions are suspected or confirmed compromised, immediately expel the adversary and dismantle all footholds. This includes terminating active sessions, revoking tokens, purging tainted conversational memory, and disabling malicious execution paths like unknown webhooks or queued jobs. The goal is to prevent any residual access so the attacker cannot continue or instantly re-enter.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-006",
        "name":"End-of-Life (EOL) of Models, Configurations and Data",
        "description":"Implements formal, verifiable technical decommissioning procedures to securely and permanently delete or dispose of AI models, configurations, and their associated data at the end of their lifecycle or upon a transfer of ownership. This technique ensures that residual data cannot be recovered and that security issues from a decommissioned system cannot be transferred to another.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-001.001",
        "name":"Foundational Credential Management",
        "description":"This sub-technique covers the standard, proactive lifecycle management and incident response for credentials associated with human users and traditional services (e.g., database accounts, long-lived service account keys). It includes essential security hygiene practices like regularly rotating secrets, as well as reactive measures such as forcing password resets and cleaning up unauthorized accounts after a compromise has been detected.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-001.002",
        "name":"Automated & Real-time Invalidation",
        "description":"This sub-technique covers the immediate, automated, and reactive side of credential eviction. It focuses on integrating security alerting with response workflows to automatically disable compromised credentials the moment they are detected. It also addresses the challenge of ensuring that revocations for stateless tokens (like JWTs) are propagated and enforced in real-time to immediately terminate an attacker's session.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-001.003",
        "name":"AI Agent & Workload Identity Revocation",
        "description":"This sub-technique covers the specialized task of revoking credentials and identities for non-human, AI-specific entities. It addresses modern, ephemeral identity types like those used by autonomous agents and containerized workloads, such as short-lived mTLS certificates, cloud workload identities (e.g., IAM Roles for Service Accounts), and SPIFFE Verifiable Identity Documents (SVIDs). The goal is to immediately evict a compromised AI workload from the trust domain.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-003.001",
        "name":"Neural Network Backdoor Detection & Removal",
        "description":"Focuses on identifying and removing backdoors embedded within neural network model parameters, including trigger-based backdoors that cause misclassification on specific inputs.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-003.002",
        "name":"Poisoned Data Detection & Cleansing",
        "description":"Identifies and removes maliciously crafted data points from training sets, vector databases, or other data stores that could influence model behavior or enable attacks.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-003.003",
        "name":"Malicious Code & Configuration Cleanup",
        "description":"Removes malicious scripts, modified configuration files, unauthorized tools, or persistence mechanisms that attackers may have introduced into the AI system infrastructure.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-003.004",
        "name":"Malicious Node Eviction in Graph Datasets",
        "description":"After a detection method identifies nodes that are likely poisoned or part of a backdoor trigger, this eviction technique systematically removes those nodes and their associated edges from the graph dataset. This cleansing action is performed before the final, clean Graph Neural Network (GNN) model is trained or retrained, ensuring the malicious artifacts and their influence are fully purged from the training process.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-006.001",
        "name":"Cryptographic Erasure & Media Sanitization",
        "description":"Employs cryptographic and physical methods to render AI data and models on storage media permanently unrecoverable. This is the core technical process for decommissioning AI assets, ensuring compliance with data protection regulations and preventing future data leakage.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-E-006.002",
        "name":"Secure Asset Transfer & Ownership Change",
        "description":"Defines the technical process for securely transferring ownership of an AI asset to another entity. This involves cryptographic verification of the transferred artifact and a corresponding secure deletion of the original asset to prevent residual security risks.",
        "tactic":"Evict"
    },
    {
        "defense_id":"AID-H-001",
        "name":"Adversarial Robustness Training",
        "description":"A set of techniques that proactively improve a model's resilience to adversarial inputs by training it with examples specifically crafted to try and fool it. This process 'vaccinates' the model against various forms of attack\u2014from subtle, full-image perturbations to localized, high-visibility adversarial patches\u2014by directly incorporating adversarial defense into the training loop, forcing the model to learn more robust and generalizable features.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-002",
        "name":"AI-Contextualized Data Sanitization & Input Validation",
        "description":"Implement rigorous validation, sanitization, and filtering mechanisms for all data fed into AI systems. This applies to training data, fine-tuning data, and live operational inputs (including user prompts for LLMs). The goal is to detect and remove or neutralize malicious content, anomalous data, out-of-distribution samples, or inputs structured to exploit vulnerabilities like prompt injection or data poisoning before they can adversely affect the model or downstream systems. For LLMs, this involves specific techniques like stripping or encoding control tokens and filtering for known injection patterns or harmful content. For multimodal systems, this includes validating and sanitizing inputs across all modalities (e.g., text, image, audio, video) and ensuring that inputs in one modality cannot be readily used to trigger vulnerabilities, bypass controls, or inject malicious content into another modality processing pathway.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003",
        "name":"Secure ML Supply Chain Management",
        "description":"Apply rigorous software supply chain security principles throughout the AI\/ML development and operational lifecycle. This involves verifying the integrity, authenticity, and security of all components, including source code, pre-trained models, datasets, ML libraries, development tools, and deployment infrastructure. The aim is to prevent the introduction of vulnerabilities, backdoors, malicious code (e.g., via compromised dependencies), or tampered artifacts into the AI system. This is critical as AI systems often rely on a complex ecosystem of third-party elements.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-004",
        "name":"Identity & Access Management (IAM) for AI Systems",
        "description":"Implement and enforce comprehensive Identity and Access Management (IAM) controls for all AI resources, including models, APIs, data stores, agentic tools, and administrative interfaces. This involves applying the principle of least privilege, strong authentication, and robust authorization to limit who and what can interact with, modify, or manage AI systems.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-005",
        "name":"Privacy-Preserving Machine Learning (PPML) Techniques",
        "description":"Employ a range of advanced cryptographic and statistical techniques during AI model training, fine-tuning, and inference to protect the privacy of sensitive information within datasets. These methods aim to prevent the leakage of individual data records, membership inference, or the reconstruction of sensitive inputs from model outputs.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-006",
        "name":"AI Output Hardening & Sanitization",
        "description":"Implement programmatic transformations, structuring, and sanitization on the raw output generated by an AI model before it is passed to a user or downstream system. This proactive control aims to enforce a safe, expected format, remove potentially exploitable content, and reduce the risk of the output itself becoming an attack vector against end-users or other system components. This is distinct from detective output monitoring; it is a preventative measure to harden the output stream itself.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-007",
        "name":"Secure & Resilient Training Process Hardening",
        "description":"Implement robust security measures to protect the integrity, confidentiality, and stability of the AI model training process itself. This involves securing the training environment (infrastructure, code, data access), continuously monitoring training jobs for anomalous behavior (e.g., unexpected resource consumption, convergence failures, unusual metric fluctuations that could indicate subtle data poisoning effects not caught by pre-filtering or direct manipulation of training code), and ensuring training reproducibility and auditability.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-008",
        "name":"Robust Federated Learning Aggregation",
        "description":"Implement and enforce secure aggregation protocols and defenses against malicious or unreliable client updates within Federated Learning (FL) architectures. This technique aims to prevent attackers controlling a subset of participating clients from disproportionately influencing, poisoning, or degrading the global model, or inferring information about other clients' data.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-009",
        "name":"AI Accelerator & Hardware Integrity",
        "description":"Implement measures to protect the physical integrity and operational security of specialized AI hardware (GPUs, TPUs, NPUs, FPGAs) and the platforms hosting them against physical tampering, side-channel attacks (power, timing, EM), fault injection, and hardware Trojans. This aims to ensure the confidentiality and integrity of AI computations and model parameters processed by the hardware.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-010",
        "name":"Transformer Architecture Defenses",
        "description":"Implement security measures specifically designed to mitigate vulnerabilities inherent in the Transformer architecture, such as attention mechanism manipulation, position embedding attacks, and risks associated with self-attention complexity. These defenses aim to protect against attacks that exploit how Transformers process and prioritize information.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-011",
        "name":"Classifier-Free Guidance Hardening",
        "description":"A set of techniques focused on hardening the Classifier-Free Guidance (CFG) mechanism in diffusion models. CFG is a core component that steers image generation towards a text prompt, but adversaries can exploit high guidance scale values to force the model to generate harmful, unsafe, or out-of-distribution content. These hardening techniques aim to control the CFG scale and its influence, preventing its misuse while preserving the model's creative capabilities.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-012",
        "name":"Graph Neural Network (GNN) Poisoning Defense",
        "description":"Implement defenses to secure Graph Neural Networks (GNNs) against data poisoning attacks that manipulate the graph structure (nodes, edges) or node features. The goal is to ensure the integrity of the graph data and the robustness of the GNN's predictions against malicious alterations.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-013",
        "name":"Reinforcement Learning (RL) Reward Hacking Prevention",
        "description":"Design and implement safeguards to prevent Reinforcement Learning (RL) agents from discovering and exploiting flaws in the reward function to achieve high rewards for unintended or harmful behaviors ('reward hacking'). This also includes protecting the reward signal from external manipulation.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-014",
        "name":"Proactive Data Perturbation & Watermarking",
        "description":"Intentionally altering or embedding signals into source data (images, text, etc.) before it is used or shared, in order to disrupt, trace, or defend against misuse in downstream AI systems.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-015",
        "name":"Ensemble Methods for Robustness",
        "description":"An architectural defense that improves a system's resilience by combining the predictions of multiple, independently trained AI models. An attacker must now successfully deceive a majority of the models in the ensemble to cause a misclassification, significantly increasing the difficulty of a successful evasion attack. This technique is applied at inference time and is distinct from training-time hardening methods.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-016",
        "name":"Certified Defenses",
        "description":"A set of advanced techniques that provide a mathematical, provable guarantee that a model's output will not change for any input within a defined 'robustness radius'. Unlike empirical defenses like standard adversarial training, which improve resilience against known attack types, certified defenses use formal methods to prove that no attack within a certain magnitude (e.g., L-infinity norm) can cause a misclassification. This is a highly specialized task that offers the highest level of assurance against evasion attacks.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-017",
        "name":"System Prompt Hardening",
        "description":"This technique focuses on the design and implementation of robust system prompts to proactively defend Large Language Models (LLMs) against prompt injection, jailbreaking, and manipulation. It involves crafting the LLM's core instructions to be clear, unambiguous, and resilient to adversarial user inputs. This is a design-time, preventative control that establishes the foundational security posture of an LLM-based application.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-018",
        "name":"Secure Agent Architecture",
        "description":"This technique covers the secure-by-design architectural principles for building autonomous AI agents. It focuses on proactively designing the agent's core components\u2014such as its reasoning loop, tool-use mechanism, state management, and action dispatchers\u2014to be inherently more robust, auditable, and resistant to manipulation. This is distinct from monitoring agent behavior; it is about building the agent securely from the ground up.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-019",
        "name":"Tool Authorization & Capability Scoping",
        "description":"Establish and enforce strict authorization and capability limits for tools invocable by an AI agent. Apply least privilege with allowlists, parameter boundaries, and mandatory structured inputs\/outputs to constrain agent capabilities and prevent unauthorized or dangerous operations even under prompt manipulation.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-020",
        "name":"Safe Fetch & Browser Sandbox for Agents",
        "description":"Impose strict controls on agent-initiated HTTP requests and web browsing to prevent SSRF, internal reconnaissance, and attacks via malicious web content. All external content is treated as untrusted and demoted to safe text before LLM consumption.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-021",
        "name":"RAG Index Hygiene & Signing",
        "description":"Implement integrity and provenance controls during RAG indexing and maintenance. Cryptographically sign chunks\/embeddings and weight content by source trust to prevent index poisoning and enable verification at retrieval time.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-022",
        "name":"AI Agent Configuration Integrity & Hardening",
        "description":"This technique establishes and enforces the integrity and security of an AI agent's instructional configurations (e.g., system prompts, operational parameters, external configuration files like `CLAUDE.md`). It employs a defense-in-depth strategy, combining client-side controls to prevent the creation of insecure configurations in development environments with cryptographic verification at runtime to ensure the agent only operates based on a trusted, untampered directive. **Crucially, this technique mandates the use of structured, non-executable formats (e.g., schema-validated JSON, YAML) for configurations, prohibiting the use of formats that could be interpreted as executable code.** This directly counters attacks where adversaries manipulate an agent's behavior by injecting malicious instructions through external or modified configuration files.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-002.001",
        "name":"Training & Fine-Tuning Data Sanitization",
        "description":"Focuses on detecting and removing poisoned samples, unwanted biases, or sensitive data from datasets before they are used for model training or fine-tuning. This pre-processing step is critical for preventing the model from learning vulnerabilities or undesirable behaviors from the outset.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-002.002",
        "name":"Inference-Time Prompt & Input Validation",
        "description":"Focuses on real-time defense against malicious inputs at the point of inference, such as prompt injection, jailbreaking attempts, or other input-based evasions. This technique acts as a guardrail for the live, operational model.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-002.003",
        "name":"Multimodal Input Sanitization",
        "description":"Focuses on the unique challenges of validating and sanitizing non-textual inputs like images, audio, and video before they are processed by a model. This includes implementing defensive transformations to remove adversarial perturbations, stripping potentially malicious metadata, and ensuring consistency across modalities to prevent cross-modal attacks.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003.001",
        "name":"Software Dependency & Package Security",
        "description":"Ensure integrity of all third-party code and libraries (Python packages, containers, build tools) used to develop and serve AI workloads.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003.002",
        "name":"Model Artifact Verification & Secure Distribution",
        "description":"Mandatory, automated acceptance criteria applied to a model artifact before promotion to production. Acts as a final security gate to ensure only trusted, verified, and safely configured models are deployed by validating their origin, integrity, provenance, and operational policies. This control assumes production never trusts public names directly; only immutable, attested bytes from an internal mirror are allowed.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003.003",
        "name":"Dataset Supply Chain Validation",
        "description":"Authenticate, checksum and licence-check every external dataset (training, fine-tuning, RAG).",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003.004",
        "name":"Hardware & Firmware Integrity Assurance",
        "description":"Verify accelerator cards, firmware and BIOS\/UEFI images are genuine and un-modified before joining an AI cluster.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003.005",
        "name":"Infrastructure as Code (IaC) Security Scanning for AI Systems",
        "description":"Covers the 'pre-deployment' phase of automatically scanning Infrastructure as Code (IaC) files (e.g., Terraform, CloudFormation, Kubernetes YAML) in the CI\/CD pipeline. This 'shift-left' security practice aims to detect and block security misconfigurations, such as insecure network paths that could undermine model supply chain security, before infrastructure is provisioned.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-003.006",
        "name":"Model SBOM & Provenance Attestation",
        "description":"Produce a model-centric SBOM that inventories model bytes (files, hashes), tokenizer, config, format, and loader code commit; bind it to the exact artifact digest via in-toto\/SLSA attestation signed with Sigstore. Verify the attestation and digest binding at admission and re-verify hashes at runtime before loading. This creates a tamper-evident content contract for the model, closing name\/namespace trust gaps.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-004.001",
        "name":"User & Privileged Access Management",
        "description":"Focuses on securing access for human users, such as developers, data scientists, and system administrators, who manage and interact with AI systems. The goal is to enforce strong authentication and granular permissions for human identities.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-004.002",
        "name":"Service & API Authentication",
        "description":"Focuses on securing machine-to-machine communication for AI services. This includes authenticating service accounts, applications, and other services that need to interact with AI model APIs, data stores, or MLOps pipelines.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-004.003",
        "name":"Secure Agent-to-Agent Communication",
        "description":"Focuses on the unique challenge of securing communications within multi-agent systems. This ensures that autonomous agents can trust each other, and that their messages cannot be spoofed, tampered with, or replayed by an adversary.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-005.001",
        "name":"Differential Privacy for AI",
        "description":"Implements differential privacy mechanisms to add calibrated noise to model training, outputs, or data queries, ensuring that individual data points cannot be identified while maintaining overall utility.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-005.002",
        "name":"Homomorphic Encryption for AI",
        "description":"Enables computation on encrypted data, allowing models to train or perform inference without ever decrypting sensitive information, providing strong cryptographic guarantees.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-005.003",
        "name":"Adaptive Data Augmentation for Membership Inference Defense",
        "description":"Employs adaptive data augmentation techniques, such as 'mixup', during the model training process to harden it against membership inference attacks (MIAs). Mixup creates new training samples by linearly interpolating between existing samples and their labels. The 'adaptive' component involves dynamically adjusting the mixup strategy during training, which enhances the model's generalization and makes it more difficult for an attacker to determine if a specific data point was part of the training set.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-005.004",
        "name":"LLM Training Data Deduplication",
        "description":"A data-centric hardening technique that involves systematically removing duplicate or near-duplicate sequences from the training datasets for Large Language Models (LLMs). This pre-processing step directly mitigates the risk of unintended memorization, where LLMs are prone to learn and regenerate specific training examples verbatim, which can lead to the leakage of sensitive or copyrighted information.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-006.001",
        "name":"Structured Output Enforcement",
        "description":"Forces LLMs to generate output that conforms to a strict, pre-defined schema (e.g., JSON, YAML) instead of free-form text. This ensures the output can be safely parsed and validated by downstream systems, preventing the generation of unintended or malicious scripts, formats, or commands.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-006.002",
        "name":"Output Content Sanitization & Validation",
        "description":"Applies security checks and sanitization to the content generated by an AI model before it is displayed to a user or passed to a downstream system. This involves escaping output to prevent injection attacks (e.g., Cross-Site Scripting, Shell Injection) and validating specific content types, such as URLs, against blocklists or safety APIs to prevent users from being directed to malicious websites.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-006.003",
        "name":"Passive AI Output Obfuscation",
        "description":"A hardening technique that intentionally reduces the precision or fidelity of an AI model's output before it is returned to an end-user or downstream system. This proactive control aims to significantly increase the difficulty of model extraction, inversion, and membership inference attacks, which often rely on precise output values (like confidence scores or logits) to reverse-engineer the model or its training data. By returning less information\u2014such as binned confidence scores, rounded numerical predictions, or only the top predicted class\u2014the utility for legitimate users is maintained while the value of the output for an attacker is drastically reduced.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-007.001",
        "name":"Secure Training Environment Provisioning",
        "description":"This sub-technique focuses on the infrastructure layer of AI security. It covers the creation of dedicated, isolated, and hardened environments for training jobs using Infrastructure as Code (IaC), least-privilege IAM roles, and, where necessary, confidential computing. The goal is to build a secure foundation for the training process, protecting it from both internal and external threats, and ensuring the confidentiality and integrity of the data and model being processed.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-007.002",
        "name":"Runtime Training Job Monitoring & Auditing",
        "description":"Focuses on instrumenting the training script itself to continuously monitor for behavioral anomalies and to create a detailed, immutable audit log. This involves real-time tracking of key training metrics (e.g., loss, gradient norms) to detect signs of instability or poisoning, and systematically logging all parameters, code versions, and data versions to ensure any training run is fully auditable and reproducible.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-007.003",
        "name":"Training Process Reproducibility",
        "description":"This sub-technique focuses on the governance and versioning aspect of securing the training process. It covers the strict version control of all inputs to a training job\u2014including source code, configuration files, dependencies, the dataset, and the container image\u2014to ensure any run can be perfectly and verifiably reproduced. This is critical for auditing, debugging incidents, and ensuring the integrity of the model's entire lifecycle.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-008.001",
        "name":"Secure Aggregation Protocols for Federated Learning",
        "description":"Employs cryptographic methods in Federated Learning (FL) to protect the privacy of individual client contributions, such as model updates or gradients. These protocols are designed so the central server can compute the aggregate (sum or average) of all client updates but cannot inspect or reverse-engineer any individual contribution. This hardens the FL process against inference attacks by the server and preserves user privacy in collaborative learning environments.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-008.002",
        "name":"Byzantine-Robust Aggregation Rules",
        "description":"A class of statistical, non-cryptographic aggregation methods designed to protect the integrity of the global model in Federated Learning. These rules identify and mitigate the impact of outlier or malicious model updates from compromised clients (Byzantine actors) by using functions like median, trimmed mean, or distance-based scoring (e.g., Krum) to filter out or down-weight anomalous contributions before they can corrupt the final aggregated model.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-009.001",
        "name":"Hardware Root of Trust & Secure Boot",
        "description":"This sub-technique focuses on ensuring that the hardware and its boot-level software start in a known, trusted state. It covers the implementation and verification of Secure Boot chains for servers equipped with AI accelerators. This process establishes a chain of trust from an immutable hardware root, ensuring that every piece of software loaded during startup\u2014from the UEFI firmware to the bootloader and operating system kernel\u2014is cryptographically signed and verified, preventing boot-level malware.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-009.002",
        "name":"Accelerator Firmware & Driver Patch Management",
        "description":"This sub-technique covers the operational lifecycle management for the software that runs directly on the AI hardware. It includes processes for monitoring for vulnerabilities in firmware and drivers for GPUs, TPUs, and other accelerators, and applying security patches in a timely, controlled manner to prevent exploitation of known vulnerabilities.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-009.003",
        "name":"Hardware Supply Chain Security",
        "description":"This sub-technique focuses on the procurement and sourcing of AI hardware. It covers vetting suppliers, verifying the authenticity of components, and contractually requiring features like side-channel attack resistance. The goal is to mitigate the risk of acquiring counterfeit, tampered, or inherently vulnerable hardware components that could be used to compromise AI systems.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-009.004",
        "name":"Accelerator Isolation & VRAM\/KV-Cache Hygiene",
        "description":"In shared compute environments, prevent data leakage across GPU jobs by clearing VRAM\/KV-cache after tasks, partitioning accelerators, and staying patched against known issues (e.g., LeftoverLocals).",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-009.005",
        "name":"Confidential Inference & Remote Attestation",
        "description":"Run inference in TEEs or confidential VMs to protect model weights, inputs, and KV-cache in encrypted memory. Verify enclave integrity via remote attestation before sending sensitive assets.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-012.001",
        "name":"Graph Data Sanitization & Provenance",
        "description":"This sub-technique covers the data-centric defenses performed on a graph before training. It focuses on analyzing the graph's structure to identify and remove anomalous nodes or edges, and on incorporating provenance information (e.g., trust scores based on data sources) to down-weight the influence of less trusted parts of the graph during model training.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-012.002",
        "name":"Robust GNN Training & Architecture",
        "description":"This sub-technique covers the model-centric defenses against GNN poisoning. It focuses on modifying the GNN's architecture (e.g., using robust aggregation functions) and the training process (e.g., applying regularization) to make the model itself inherently more resilient to the effects of malicious data or structural perturbations.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-012.003",
        "name":"Certified GNN Robustness",
        "description":"This sub-technique covers the advanced, formal verification approach to defending Graph Neural Networks (GNNs). It provides a mathematical guarantee that a model's prediction for a specific node will remain unchanged even if an attacker adds or removes up to a certain number of edges in the graph. This 'certified radius' of robustness represents a distinct, high-assurance implementation path against structural poisoning attacks.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-013.001",
        "name":"Robust Reward Function Engineering",
        "description":"This subtechnique covers the direct design and engineering of the reward function itself to be inherently less exploitable. By defining multiple, sometimes competing, objectives and explicitly penalizing undesirable side effects, the reward function provides a more holistic and robust incentive structure that is harder for an agent to 'game' or hack.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-013.002",
        "name":"Human-in-the-Loop Reward Learning",
        "description":"This subtechnique covers methods for deriving robust reward functions from human feedback, rather than hand-crafting them. This includes techniques like preference-based learning and Inverse Reinforcement Learning (IRL) where the model learns the reward function from expert demonstrations. This is particularly useful for complex behaviors that are difficult to specify with a mathematical formula but are intuitive for a human to judge.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-013.003",
        "name":"Potential-Based Reward Shaping",
        "description":"This subtechnique covers the provably safe method of adding dense, intermediate rewards to guide an agent towards a goal without changing the optimal policy. It helps speed up learning in environments with sparse rewards and can guide the agent away from unsafe states, effectively hardening the learning process against some forms of reward hacking by making the intended path clearer.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-014.001",
        "name":"Digital Content & Data Watermarking",
        "description":"This subtechnique covers the methods for embedding robust, imperceptible signals into various data types (including images, audio, video, text, and code) for the purposes of tracing provenance, detecting misuse, or identifying AI-generated content. The watermark is designed to be resilient to common transformations, allowing an owner to prove that a piece of content originated from their system even after it has been distributed or modified.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-014.002",
        "name":"Adversarial Data Cloaking",
        "description":"This subtechnique covers the approach of adding small, targeted, and often imperceptible perturbations to source data. Unlike watermarking, which aims for a signal to be robustly detected, cloaking aims to disrupt or 'cloak' the data from being effectively used by specific downstream AI models. This is a proactive defense to sabotage the utility of stolen data for malicious purposes like deepfake generation or unauthorized model training.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-019.001",
        "name":"Tool Parameter Constraint & Schema Validation",
        "description":"Define strict, machine-readable schemas for each agent tool's input parameters and enforce validation before execution (types, formats, ranges, enums). Prevents malicious parameter injection (command\/SQL injection, path traversal).",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-019.002",
        "name":"Policy-Based Access Control",
        "description":"Externalize authorization decisions for tool usage with a policy engine (e.g., OPA), enabling context-aware, stateful rules that decouple policy from application code.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-020.001",
        "name":"URL Normalization & Allowlist Filtering",
        "description":"Create a safe HTTP wrapper that normalizes URLs, enforces scheme\/domain allowlists, resolves DNS and blocks private\/internal IP ranges to prevent SSRF.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-020.002",
        "name":"Secure HTML Rendering & Content Demotion",
        "description":"Strip scripts, styles, iframes, and active content; extract plain text before passing to LLM to mitigate stored XSS and indirect prompt injection.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-021.001",
        "name":"Chunk-Level Integrity Signing",
        "description":"Compute and store a cryptographic hash or digital signature per chunk at ingestion; verify on retrieval to detect tampering.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-021.002",
        "name":"Source Reputation Weighting",
        "description":"Assign and store per-chunk reputation scores based on source trust. Re-rank retrievals by combining similarity and reputation to bias toward trusted sources.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-022.001",
        "name":"Client-Side Configuration Enforcement",
        "description":"Proactively prevents the creation and use of insecure AI agent configurations on developer endpoints. This is a 'shift-left' defense that uses endpoint security tools, policy-as-code scanners, and local development guardrails to block high-risk settings before they can ever be committed to source control or executed.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-H-022.002",
        "name":"Runtime Integrity Verification",
        "description":"Ensures that an AI agent, at the moment of execution, loads and operates on a configuration that is cryptographically verified to be authentic and untampered. This is a critical runtime check that serves as a final guardrail, protecting the agent even if client-side or repository controls fail. It forms a verifiable trust chain from the secure build pipeline to the running agent.",
        "tactic":"Harden"
    },
    {
        "defense_id":"AID-I-001",
        "name":"AI Execution Sandboxing & Runtime Isolation",
        "description":"Execute AI models, autonomous agents, or individual AI tools and plugins within isolated environments such as sandboxes, containers, or microVMs. These environments must be configured with strict limits on resources, permissions, and network connectivity. The primary goal is that if an AI component is compromised or behaves maliciously, the impact is confined to the isolated sandbox, preventing harm to the host system or lateral movement.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-002",
        "name":"Network Segmentation & Isolation for AI Systems",
        "description":"Implement network segmentation and microsegmentation strategies to isolate AI systems and their components (e.g., training environments, model serving endpoints, data stores, agent control planes) from general corporate networks and other critical IT\/OT systems. This involves enforcing strict communication rules through firewalls, proxies, and network policies to limit an attacker's ability to pivot from a compromised AI component to other parts of the network, or to exfiltrate data to unauthorized destinations. This technique reduces the \\\"blast radius\\\" of a security incident involving an AI system.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-003",
        "name":"Quarantine & Throttling of AI Interactions",
        "description":"Implement mechanisms to automatically or manually isolate, rate-limit, or place into a restricted \\\"safe mode\\\" specific AI system interactions when suspicious activity is detected. This could apply to individual user sessions, API keys, IP addresses, or even entire AI agent instances. The objective is to prevent potential attacks from fully executing, spreading, or causing significant harm by quickly containing or degrading the capabilities of the suspicious entity. This is an active response measure triggered by detection systems.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-004",
        "name":"Agent Memory & State Isolation",
        "description":"Specifically for agentic AI systems, implement mechanisms to isolate and manage the agent's memory (e.g., conversational context, short-term state, knowledge retrieved from vector databases) and periodically reset or flush it. This defense aims to prevent malicious instructions, poisoned data, or exploited states (e.g., a \\\"jailbroken\\\" state) from persisting across multiple interactions, sessions, or from affecting other unrelated agent tasks or instances. It helps to limit the temporal scope of a successful manipulation.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-005",
        "name":"Emergency \"Kill-Switch\" \/ AI System Halt",
        "description":"Establish and maintain a reliable, rapidly invokable mechanism to immediately halt, disable, or severely restrict the operation of an AI model or autonomous agent if it exhibits confirmed critical malicious behavior, goes \\\"rogue\\\" (acts far outside its intended parameters in a harmful way), or if a severe, ongoing attack is detected and other containment measures are insufficient. This is a last-resort containment measure designed to prevent catastrophic harm or further compromise.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-006",
        "name":"Malicious Participant Isolation in Federated Unlearning",
        "description":"Identifies and logically isolates the influence of malicious clients within a Federated Learning (FL) system, particularly during a machine unlearning or model restoration process. Once identified, the malicious participants' data contributions and model updates are excluded from the unlearning or retraining calculations. This technique is critical for preventing attackers from sabotaging the model recovery process and ensuring the final restored model is not corrupted.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-007",
        "name":"Client-Side AI Execution Isolation",
        "description":"This technique focuses on containing a compromised or malicious client-side model, preventing it from accessing sensitive data from other browser tabs, the underlying operating system, or other applications on the user's device. It addresses the unique security challenges of AI models that execute in untrusted environments like a user's web browser or mobile application. The goal is to ensure that even if a model is compromised, its impact is strictly confined to its own sandbox.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-001.001",
        "name":"Container-Based Isolation",
        "description":"Utilizes container technologies like Docker or Kubernetes to package and run AI workloads in isolated user-space environments. This approach provides process and filesystem isolation and allows for resource management and network segmentation.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-001.002",
        "name":"MicroVM & Low-Level Sandboxing",
        "description":"Employs lightweight Virtual Machines (MicroVMs) or kernel-level sandboxing technologies to provide a stronger isolation boundary than traditional containers. This is critical for running untrusted code or highly sensitive AI workloads.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-001.003",
        "name":"Ephemeral Single-Use Sandboxes for Tools",
        "description":"Run tool executions inside strongly isolated, single-use sandboxes (e.g., microVMs). Destroy the environment immediately after one invocation to prevent persistence and cross-session contamination.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-001.004",
        "name":"Seccomp-bpf & Network Egress Restrictions",
        "description":"Minimize kernel\/system call surface and restrict outbound network destinations for sandboxed executions to reduce post-exploitation blast radius.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-I-001.005",
        "name":"Pre-Execution Behavioral Analysis in Ephemeral Sandboxes",
        "description":"This proactive defense technique subjects any AI-generated executable artifact (e.g., scripts, binaries, container images created by an agent) to mandatory behavioral analysis within a short-lived, strongly isolated sandbox (such as a microVM) *before* it is deployed or executed in a production context. This pre-execution security gate applies to artifacts originating from both automated CI\/CD pipelines and interactive developer IDEs, serving as a final vetting step to contain threats from malicious AI-generated code before they can have any impact.",
        "tactic":"Isolate"
    },
    {
        "defense_id":"AID-M-001",
        "name":"AI Asset Inventory & Mapping",
        "description":"Systematically catalog and map all AI\/ML assets, including models (categorized by type, version, deployment location, and ownership), datasets (training, validation, testing, and operational), data pipelines, and APIs. This process includes mapping their configurations, data flows (sources, transformations, destinations), and interdependencies (e.g., reliance on third-party APIs, upstream data providers, or specific libraries). The goal is to achieve comprehensive visibility into all components that constitute the AI ecosystem and require protection. This technique is foundational as it underpins the ability to apply targeted security controls and assess risk accurately.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-002",
        "name":"Data Provenance & Lineage Tracking",
        "description":"Establish and maintain verifiable records of the origin, history, and transformations of data used in AI systems, particularly training and fine-tuning data. This includes tracking model updates and their associated data versions. The objective is to ensure the trustworthiness and integrity of data and models by knowing their complete lifecycle, from source to deployment, and to facilitate auditing and incident investigation. This often involves cryptographic methods like signing or checksumming datasets and subunits and models at critical stages.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003",
        "name":"Model Behavior Baseline & Documentation",
        "description":"Establish, document, and maintain a comprehensive baseline of expected AI model behavior. This includes defining its intended purpose, architectural details, training data characteristics, operational assumptions, limitations, and key performance metrics (e.g., accuracy, precision, recall, output distributions, latency, confidence scores) under normal conditions. This documentation, often in the form of model cards, and the established behavioral baseline serve as a reference to detect anomalies, drift, or unexpected outputs that might indicate an attack or system degradation, and to inform risk assessments and incident response.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-004",
        "name":"AI Threat Modeling & Risk Assessment",
        "description":"Systematically identify, analyze, and prioritize potential AI-specific threats and vulnerabilities for each AI component (e.g., data, models, algorithms, pipelines, agentic capabilities, APIs) throughout its lifecycle. This process involves understanding how an adversary might attack the AI system and assessing the potential impact of such attacks. The outcomes guide the design of appropriate defensive measures and inform risk management strategies. This proactive approach is essential for building resilient AI systems.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-005",
        "name":"AI Configuration Benchmarking & Secure Baselines",
        "description":"Establish, document, maintain, and regularly audit secure configurations for all components of AI systems. This includes the underlying infrastructure (cloud instances, GPU clusters, networks), ML libraries and frameworks, agent runtimes, MLOps pipelines, and specific settings within AI platform APIs (e.g., LLM function access). Configurations are benchmarked against industry standards (e.g., CIS Benchmarks, NIST SSDF), vendor guidance, and internal security policies to identify and remediate misconfigurations that could be exploited by attackers.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-006",
        "name":"Human-in-the-Loop (HITL) Control Point Mapping",
        "description":"Systematically identify, document, map, and validate all designed human intervention, oversight, and control points within AI systems. This is especially critical for agentic AI and systems capable of high-impact autonomous decision-making. The process includes defining the triggers, procedures, required operator training, and authority levels for human review, override, or emergency system halt. The goal is to ensure that human control can be effectively, safely, and reliably exercised when automated defenses fail, novel threats emerge, or ethical boundaries are approached.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-007",
        "name":"AI Use Case & Safety Boundary Modeling",
        "description":"This technique involves the formal, technical documentation and validation of an AI system's intended purpose, operational boundaries, and ethical guardrails. It translates abstract governance policies into concrete, machine-readable artifacts and automated tests that model the system's safety posture. The goal is to proactively define and enforce the AI's scope of acceptable use, assess it for fairness and bias, and analyze its potential for misuse, creating a verifiable record for security, compliance, and responsible AI assurance. This serves as the business and ethical context counterpart to the technical threat model (AID-M-004).",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-001.001",
        "name":"AI Component & Infrastructure Inventory",
        "description":"Systematically catalogs all AI\/ML assets, including models (categorized by type, version, and ownership), datasets, software components, and the specialized hardware they run on (e.g., GPUs, TPUs). This technique focuses on creating a dynamic, up-to-date inventory to provide comprehensive visibility into all components that constitute the AI ecosystem, which is a prerequisite for accurate risk assessment and the application of targeted security controls.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-001.002",
        "name":"AI System Dependency Mapping",
        "description":"Systematically identifies and documents all components and services that an AI system depends on to function correctly. This includes direct software libraries, transitive dependencies, external data sources, third-party APIs, and other internal AI models or microservices. This dependency map is crucial for understanding the complete supply chain attack surface and for performing comprehensive security assessments.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-002.001",
        "name":"Data & Artifact Versioning",
        "description":"Implements systems and processes to version control datasets and model artifacts, treating them with the same rigor as source code. By tracking every version of a data file and linking it to specific code commits, this technique ensures perfect reproducibility, provides an auditable history of changes, and enables rapid rollbacks to a known-good state, which is critical for recovering from data corruption or poisoning incidents.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-002.002",
        "name":"Cryptographic Integrity Verification",
        "description":"Employs cryptographic hashing and digital signatures to create and verify a tamper-evident chain of custody for AI artifacts. This technique ensures that datasets, models, and other critical files are authentic and have not been altered or corrupted at any point in their lifecycle, from creation and storage to deployment and use. It provides a strong mathematical guarantee of artifact integrity.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-002.003",
        "name":"Third-Party Data Vetting",
        "description":"Implements a formal, security-focused process for onboarding any external or third-party datasets. This technique involves a combination of procedural checks (source reputation, licensing) and technical scans (PII detection, integrity verification, statistical profiling) to identify and mitigate risks before untrusted data is introduced into the organization's AI ecosystem.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.001",
        "name":"Model Card & Datasheet Generation",
        "description":"A systematic process of creating and maintaining standardized documentation for AI models (Model Cards) and datasets (Datasheets). This documentation captures crucial metadata, including the model's intended use cases, limitations, performance metrics, fairness evaluations, ethical considerations, and details about the data's provenance and characteristics. This ensures transparency, enables responsible governance, and provides a foundational reference for security audits and risk assessments.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.002",
        "name":"Performance & Operational Metric Baselining",
        "description":"Establishes a quantitative, empirical baseline of a model's expected behavior under normal conditions. This involves calculating and recording two types of metrics: 1) key performance indicators (e.g., accuracy, precision, F1-score) on a trusted, 'golden' dataset, and 2) operational metrics (e.g., inference latency, confidence scores, output distributions) derived from simulated or live traffic. This documented baseline serves as the ground truth for drift detection, anomaly detection, and ongoing performance monitoring.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.003",
        "name":"Explainability (XAI) Output Baselining",
        "description":"Establishes a baseline of normal or expected outputs from eXplainable AI (XAI) methods for a given AI model. By generating and documenting typical explanations (e.g., feature attributions, decision rules) for a diverse set of known, benign inputs, this technique creates a reference point to detect future anomalies. A significant deviation from this baseline can indicate that an attacker is attempting to manipulate or mislead the explanation method itself to conceal malicious activity, as investigated by AID-D-006.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.004",
        "name":"Agent Goal & Mission Baselining",
        "description":"Specifically for autonomous or agentic AI, this technique involves formally defining, documenting, and cryptographically signing the agent's core mission, objectives, operational constraints, and goal hierarchy. This signed 'mission directive' serves as a trusted, immutable baseline. It is a critical prerequisite for runtime monitoring systems (like AID-D-010) to detect goal manipulation, unauthorized deviations, or emergent behaviors that contradict the agent's intended purpose.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.005",
        "name":"Generative Model Inversion for Anomaly Pre-screening",
        "description":"Utilizes a generative model (e.g., a Generative Adversarial Network - GAN) to establish a baseline of 'normal' data characteristics. An input, such as an image, is projected into the model's latent space to find a vector that best reconstructs the input. A high reconstruction error suggests the input is anomalous, out-of-distribution, or potentially a synthetic deepfake not created by a similar generative process. This technique models the expected data fidelity to pre-screen inputs for potential threats.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.006",
        "name":"Graph Energy Analysis for GNN Robustness",
        "description":"Utilizes metrics derived from a graph's adjacency matrix, such as graph subspace energy, as a quantifiable indicator of a Graph Neural Network's (GNN) robustness to adversarial topology perturbations. By modeling and baselining these structural properties, this technique can guide the development of more inherently resilient GNNs, for instance, by enhancing adversarial training to generate perturbations that are not only effective but also structurally significant according to the energy metric.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-003.007",
        "name":"Self-Supervised Discrepancy Analysis for GNN Backdoor Defense",
        "description":"Employs self-supervised learning to train an auxiliary Graph Neural Network (GNN) model that learns the intrinsic semantic information and attribute importance of nodes without using potentially poisoned labels. This process creates a trusted baseline model of 'normal' node characteristics. This baseline is then used as a reference to detect discrepancies\u2014such as semantic drift or attribute over-emphasis\u2014in a primary GNN trained on the potentially compromised data, which is a foundational step for identifying and mitigating backdoor attacks.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-005.001",
        "name":"Design - Secure Configuration Baseline Development",
        "description":"Covers the 'design' phase of creating and documenting secure, hardened templates and configurations for all AI system components, based on industry benchmarks. This proactive technique involves defining 'golden standard' configurations for infrastructure, containers, and AI platforms to ensure that systems are secure by default, systematically reducing the attack surface by eliminating common misconfigurations before deployment.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-005.002",
        "name":"Pre-Deployment - Infrastructure as Code (IaC) Security Scanning",
        "description":"Covers the 'pre-deployment' phase of automatically scanning Infrastructure as Code (IaC) files (e.g., Terraform, CloudFormation, Bicep, Kubernetes YAML) in the CI\/CD pipeline. This 'shift-left' security practice aims to detect and block security misconfigurations, policy violations, and hardcoded secrets before insecure infrastructure is ever provisioned in a live environment.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-005.003",
        "name":"Post-Deployment - Configuration Drift & Posture Monitoring",
        "description":"Covers the 'post-deployment' phase of using Cloud Security Posture Management (CSPM) tools and custom scripts to continuously monitor live cloud environments. This technique aims to detect and alert on 'configuration drift'\u2014unauthorized or accidental changes that cause a system to deviate from its established secure baseline\u2014providing a real-time view of the security posture for all AI system components.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-006.001",
        "name":"HITL Checkpoint Design & Documentation",
        "description":"This sub-technique covers the initial development phase of implementing Human-in-the-Loop controls. It involves formally defining the specific triggers that require human intervention in code and configuration, implementing the technical hooks for the AI agent to pause and await a decision, and creating the clear Standard Operating Procedures (SOPs) that operators will follow when an intervention is required.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-006.002",
        "name":"HITL Operator Training & Readiness Testing",
        "description":"Covers the human and procedural readiness aspects of a Human-in-the-Loop (HITL) system. This technique involves developing comprehensive training programs and running simulated emergency scenarios ('fire drills') for human operators. It also includes regularly auditing and testing the technical HITL mechanisms to ensure both operator preparedness and end-to-end functionality, confirming that human control can be asserted effectively and reliably when needed.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-M-006.003",
        "name":"HITL Escalation & Activity Monitoring",
        "description":"Covers the live operational and security aspects of a Human-in-the-Loop (HITL) system. This technique involves defining and implementing the technical escalation paths for undecided or unhandled intervention requests and ensuring that all HITL activations, operator decisions, and system responses are securely logged. This provides a comprehensive audit trail for forensic analysis and real-time monitoring to detect anomalous operator behavior or high-frequency intervention events.",
        "tactic":"Model"
    },
    {
        "defense_id":"AID-R-001",
        "name":"Secure AI Model Restoration & Retraining",
        "description":"Name of the model to roll back",
        "tactic":"Restore"
    },
    {
        "defense_id":"AID-R-002",
        "name":"Data Integrity Recovery for AI Systems",
        "description":"Restore the integrity of any datasets used by or generated by AI systems that were corrupted, tampered with, or maliciously altered during a security incident. This includes training data, validation data, vector databases for RAG, embeddings stores, configuration data, or logs of AI outputs. Recovery typically involves reverting to known-good backups, using data validation tools to identify and correct inconsistencies, or, in some cases, reconstructing data if backups are insufficient or also compromised.",
        "tactic":"Restore"
    },
    {
        "defense_id":"AID-R-003",
        "name":"Secure Session & Identity Re-establishment",
        "description":"After an eviction (AID-E-005) is complete, this technique re-establishes clean, trusted interactions for users and AI agents. It focuses on hardening the recovery process by enforcing strong re-authentication (MFA\/step-up), ensuring modern TLS, restoring clean conversational context from trusted snapshots, and progressively re-enabling capabilities. This includes clear communication with legitimate users and maintaining heightened monitoring to detect and prevent immediate re-compromise.",
        "tactic":"Restore"
    },
    {
        "defense_id":"AID-R-004",
        "name":"Post-Incident Review, Hardening & Communication",
        "description":"Following recovery from a security incident, conduct a thorough, blameless review of the attack to identify the root cause. Based on these lessons learned, reinforce security controls, update threat models, perform targeted testing to validate fixes, and communicate the incident details to relevant internal and external stakeholders. This comprehensive process ensures that vulnerabilities are addressed, system resilience is improved, and knowledge is shared to prevent recurrence and improve collective defense.",
        "tactic":"Restore"
    },
    {
        "defense_id":"AID-R-005",
        "name":"Rapid Vector Index Rollback & Quarantine",
        "description":"Implement fast, reliable recovery procedures for a Retrieval-Augmented Generation (RAG) system's vector index after a poisoning attack has been detected. This technique relies on versioning the index to enable rapid rollback to the last known-good state. It also includes mechanisms to quarantine suspicious document chunks, removing them from the active index for forensic analysis while maintaining the availability of the rest of the system. The goal is to minimize service disruption and ensure the restored index is clean and trustworthy.",
        "tactic":"Restore"
    },
    {
        "defense_id":"AID-R-001.001",
        "name":"Versioned Model Rollback & Restoration",
        "description":"Name of the model to roll back",
        "tactic":"Restore"
    },
    {
        "defense_id":"AID-R-001.002",
        "name":"Model Retraining for Remediation",
        "description":"This sub-technique covers the restoration workflow for when a model's integrity has been compromised by its training data (e.g., via data poisoning or backdoor attacks). It involves a multi-stage process: first, identifying and removing the malicious data or client influence; second, retraining a new model from scratch using only the resulting sanitized dataset; and third, validating that the new model is both secure and performant. This approach is more comprehensive than a simple rollback and is necessary when no trusted model backup exists.",
        "tactic":"Restore"
    }
]